{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d061a6cfa56efd4",
   "metadata": {},
   "source": [
    "# Review Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e21a28dfc997bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:28:22.848678Z",
     "start_time": "2024-06-13T07:28:22.837540Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:25:46.000167Z",
     "start_time": "2024-06-13T07:25:45.992661Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "print(a[0:])\n",
    "print(a[0:-1])\n",
    "print(a[0:-0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255f74978db65554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:26:55.986835Z",
     "start_time": "2024-06-13T07:26:55.971526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5062f525b8f797e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:27:15.679149Z",
     "start_time": "2024-06-13T07:27:15.673806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "b = iter(a)\n",
    "for j in b:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b154e93b3b8b4d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:29:06.030836Z",
     "start_time": "2024-06-13T07:29:06.015772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4142135623730951, 2.0, 2.449489742783178, 2.8284271247461903, 3.1622776601683795]\n"
     ]
    }
   ],
   "source": [
    "c = [np.sqrt(i) for i in a if i % 2 == 0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c614c178e2c7c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:31:11.895948Z",
     "start_time": "2024-06-13T07:31:11.883431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "d = {'Genshin':6,'Impact':7 }\n",
    "print(d['Genshin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bf7c2b3cfc4e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T07:34:26.951261Z",
     "start_time": "2024-06-13T07:34:26.933353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1^_^', '2^_^', '3^_^', '4^_^', '5^_^', '6^_^', '7^_^', '8^_^', '9^_^', '10^_^']\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return str(x)+'^_^'\n",
    "\n",
    "lol = map(f, a)\n",
    "print(list(lol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a7aaf8090fd0eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T08:06:33.004445Z",
     "start_time": "2024-06-13T08:06:32.996236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fibonacci = [1, 1, 2, 3, 5, 8, 13, 21, 34, 55]\n",
    "odd_fibonacci = list(filter(lambda x: x % 2!= 0, fibonacci))\n",
    "print(odd_fibonacci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b3217840fb99f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T12:31:04.483262Z",
     "start_time": "2024-06-14T12:31:04.468527Z"
    }
   },
   "outputs": [],
   "source": [
    "# Program to multiply two matrices using list comprehension\n",
    "X=[[12,7,3],[4,5,6],[7,8,9]] # 3x3 matrix\n",
    "Y=[[5,8,1,2],[6,7,3,0],[4,5,9,1]] # 3x4 matrix\n",
    "result =[[0,0,0,0],[0,0,0,0],[0,0,0,0]] # result is 3x4\n",
    "result =[[sum(a*b for a,b in zip(X_row,Y_col))\n",
    "         for Y_col in zip(*Y)] # 解包再zip == 转置for X row in X]\n",
    "         for X_row in X]\n",
    "for r in result:\n",
    "    print (r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36696a40a19ad73b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T13:08:30.200274Z",
     "start_time": "2024-06-14T13:08:30.182967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20\n"
     ]
    }
   ],
   "source": [
    "print(10, 20)\n",
    "#print函数可以自动加空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ff67105b6535f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T03:56:01.933040Z",
     "start_time": "2024-06-16T03:55:59.750698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1\n",
      " 0 1 0 1 0 0 1 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#手搓神经网络：ppt lec16 54页开始\n",
    "import numpy as np\n",
    "import torch\n",
    "num_obs = 50\n",
    "x_mat_1 = np.random.uniform(-1, 1, size = (num_obs,2))\n",
    "x_mat_bias = np.ones((num_obs, 1))\n",
    "x_mat_full = np.concatenate((x_mat_1, x_mat_bias), axis=1)\n",
    "\n",
    "y = ((np.abs(x_mat_full[:,0])+np.abs(x_mat_full[:,1]))<1).astype(int)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442da99c85c35ec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T04:11:05.261268Z",
     "start_time": "2024-06-16T04:11:05.039374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, log loss is 14.6151, accuracy is 0.4\n",
      "Iteration 200, log loss is 11.9342, accuracy is 0.62\n",
      "Iteration 400, log loss is 11.8832, accuracy is 0.62\n",
      "Iteration 600, log loss is 11.8518, accuracy is 0.62\n",
      "Iteration 800, log loss is 11.8279, accuracy is 0.62\n",
      "Iteration 1000, log loss is 11.8091, accuracy is 0.62\n",
      "Iteration 1200, log loss is 11.7935, accuracy is 0.62\n",
      "Iteration 1400, log loss is 11.7801, accuracy is 0.62\n",
      "Iteration 1600, log loss is 11.7683, accuracy is 0.62\n",
      "Iteration 1800, log loss is 11.7573, accuracy is 0.62\n",
      "Iteration 2000, log loss is 11.7470, accuracy is 0.62\n",
      "Iteration 2200, log loss is 11.7369, accuracy is 0.62\n",
      "Iteration 2400, log loss is 11.7269, accuracy is 0.62\n",
      "Iteration 2600, log loss is 11.7166, accuracy is 0.62\n",
      "Iteration 2800, log loss is 11.7061, accuracy is 0.62\n",
      "Iteration 3000, log loss is 11.6950, accuracy is 0.62\n",
      "Iteration 3200, log loss is 11.6833, accuracy is 0.62\n",
      "Iteration 3400, log loss is 11.6708, accuracy is 0.62\n",
      "Iteration 3600, log loss is 11.6574, accuracy is 0.62\n",
      "Iteration 3800, log loss is 11.6430, accuracy is 0.62\n",
      "Iteration 4000, log loss is 11.6274, accuracy is 0.62\n",
      "Iteration 4200, log loss is 11.6105, accuracy is 0.62\n",
      "Iteration 4400, log loss is 11.5921, accuracy is 0.62\n",
      "Iteration 4600, log loss is 11.5722, accuracy is 0.62\n",
      "Iteration 4800, log loss is 11.5505, accuracy is 0.62\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def forward_pass(W_1, W_2):\n",
    "    global x_mat\n",
    "    global y\n",
    "    global num_\n",
    "    z_2 = np.dot(x_mat, W_1)\n",
    "    a_2 = sigmoid(z_2)\n",
    "    z_3 = np.dot(a_2, W_2)\n",
    "    y_pred = sigmoid(z_3).reshape((len(x_mat),))\n",
    "    \n",
    "    #backprop\n",
    "    J_z_3_grad = -y + y_pred\n",
    "    J_W_2_grad = np.dot(J_z_3_grad, a_2)\n",
    "    a_2_z_2_grad = sigmoid(z_2)*(1-sigmoid(z_2))\n",
    "    J_W_1_grad = (np.dot((J_z_3_grad).reshape(-1,1), W_2.reshape(-1,1).T)*a_2_z_2_grad).T.dot(x_mat).T\n",
    "    \n",
    "    gradient = (J_W_1_grad, J_W_2_grad)\n",
    "    \n",
    "    return y_pred, gradient\n",
    "\n",
    "W_1 = np.random.uniform(-1, 1, size=(3,4))\n",
    "W_2 = np.random.uniform(-1, 1, size=(4))\n",
    "num_iter = 5000\n",
    "learning_rate = 0.001\n",
    "x_mat = x_mat_full\n",
    "\n",
    "loss_vals, accuracies = [], []\n",
    "for i in range(num_iter):\n",
    "    y_pred, (J_W_1_grad, J_W_2_grad) = forward_pass(W_1, W_2)\n",
    "    \n",
    "    W_1 = W_1 - learning_rate * J_W_1_grad\n",
    "    W_2 = W_2 - learning_rate * J_W_2_grad\n",
    "    \n",
    "    curr_loss = np.sum((y - y_pred)**2)\n",
    "    loss_vals.append(curr_loss)\n",
    "    acc = np.sum((y_pred>=.5) == y)/num_obs\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "    if((i%200)==0):\n",
    "        print('Iteration {}, log loss is {:.4f}, accuracy is {}'.format(i, curr_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953a07add2c9575f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T04:23:04.725862Z",
     "start_time": "2024-06-16T04:23:04.709676Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward(W_1, W_2, X, Y):\n",
    "    z_2 = np.dot(X, W_1)\n",
    "    a_2 = sigmoid(z_2)\n",
    "    z_3 = np.dot(a_2, W_2)\n",
    "    y_pred = sigmoid(z_3)\n",
    "\n",
    "    # Compute gradients for backpropagation\n",
    "    J_z_3_grad = J_z_3_grad = y_pred - Y # derivative of cross entropy loss with respect to z_3\n",
    "    J_W_2_grad = a_2.T @ J_z_3_grad\n",
    "    J_a_2_grad = J_z_3_grad @ W_2.T\n",
    "    a_2_z_2_grad = a_2 * (1 - a_2) # derivative of sigmoid function\n",
    "    J_z_2_grad = J_a_2_grad * a_2_z_2_grad\n",
    "    J_W_1_grad = X.T @ J_z_2_grad\n",
    "\n",
    "    return y_pred, (J_W_1_grad, J_W_2_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93089b36b25668",
   "metadata": {},
   "source": [
    "在神经网络的反向传播过程中，计算梯度是为了通过梯度下降法更新网络中的权重参数。下面是对提到的几个变量的解释：\n",
    "\n",
    "J_z_3_grad:\n",
    "这个变量代表了损失函数 \n",
    "𝐽\n",
    "J 关于第三层（输出层）的加权输入 \n",
    "𝑧\n",
    "3\n",
    "z \n",
    "3\n",
    "​\n",
    "  的梯度。直观上讲，它描述了输出层的加权输入值 \n",
    "𝑧\n",
    "3\n",
    "z \n",
    "3\n",
    "​\n",
    "  的微小变化如何影响最终的损失函数 \n",
    "𝐽\n",
    "J。在计算中，这是通过链式法则计算得到的，包含了损失函数关于输出层激活值 \n",
    "𝑦\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    "  的导数（这里是交叉熵损失的导数），以及 \n",
    "𝑦\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    "  关于 \n",
    "𝑧\n",
    "3\n",
    "z \n",
    "3\n",
    "​\n",
    "  的导数（即sigmoid函数的导数）。\n",
    "\n",
    "J_W_2_grad:\n",
    "这个变量代表了损失函数 \n",
    "𝐽\n",
    "J 关于第二层到第三层（输出层）之间的权重矩阵 \n",
    "𝑊\n",
    "2\n",
    "W \n",
    "2\n",
    "​\n",
    "  的梯度。换句话说，它告诉我们如果稍微改变 \n",
    "𝑊\n",
    "2\n",
    "W \n",
    "2\n",
    "​\n",
    "  中的某个权重值，损失函数 \n",
    "𝐽\n",
    "J 将如何变化。在反向传播算法中，这个梯度是通过将 J_z_3_grad 与第二层的激活值 \n",
    "𝑎\n",
    "2\n",
    "a \n",
    "2\n",
    "​\n",
    "  相乘（矩阵乘法）得到的，反映了第二层输出对输出层加权输入的影响，进而影响最终的损失。\n",
    "\n",
    "J_a_2_grad:\n",
    "虽然在你提供的代码中直接计算了 J_a_2_grad，但实际上它代表了损失函数 \n",
    "𝐽\n",
    "J 关于第二层的激活值 \n",
    "𝑎\n",
    "2\n",
    "a \n",
    "2\n",
    "​\n",
    "  的梯度。这是在计算 \n",
    "𝑊\n",
    "1\n",
    "W \n",
    "1\n",
    "​\n",
    "  的梯度时需要用到的中间结果，用于理解第二层激活值的变化如何通过 \n",
    "𝑊\n",
    "2\n",
    "W \n",
    "2\n",
    "​\n",
    "  影响 \n",
    "𝑧\n",
    "3\n",
    "z \n",
    "3\n",
    "​\n",
    " ，从而影响损失函数 \n",
    "𝐽\n",
    "J。\n",
    "\n",
    "J_W_1_grad:\n",
    "这个变量代表了损失函数 \n",
    "𝐽\n",
    "J 关于第一层到第二层之间的权重矩阵 \n",
    "𝑊\n",
    "1\n",
    "W \n",
    "1\n",
    "​\n",
    "  的梯度。它是通过将 J_z_2_grad 与输入 \n",
    "𝑋\n",
    "X 相乘得到的，反映了输入层到隐藏层权重对损失函数的影响。\n",
    "\n",
    "在神经网络的训练中，这些梯度被用来更新网络中的权重，以最小化损失函数 \n",
    "𝐽\n",
    "J。具体来说，权重更新公式通常如下所示：\n",
    "\n",
    "𝑊\n",
    "𝑛\n",
    "𝑒\n",
    "𝑤\n",
    "=\n",
    "𝑊\n",
    "𝑜\n",
    "𝑙\n",
    "𝑑\n",
    "−\n",
    "𝜂\n",
    "⋅\n",
    "∇\n",
    "𝐽\n",
    "W \n",
    "new\n",
    "​\n",
    " =W \n",
    "old\n",
    "​\n",
    " −η⋅∇J\n",
    "其中 \n",
    "𝜂\n",
    "η 是学习率，\n",
    "∇\n",
    "𝐽\n",
    "∇J 是对应的权重梯度（例如，对于 \n",
    "𝑊\n",
    "2\n",
    "W \n",
    "2\n",
    "​\n",
    " ，\n",
    "∇\n",
    "𝐽\n",
    "∇J 就是 J_W_2_grad）。\n",
    "\n",
    "那么a_2_z_2_grad是什么意思？\n",
    "\n",
    "a_2_z_2_grad 表示的是第二层激活值 \n",
    "𝑎\n",
    "2\n",
    "a \n",
    "2\n",
    "​\n",
    "  关于第二层的加权输入 \n",
    "𝑧\n",
    "2\n",
    "z \n",
    "2\n",
    "​\n",
    "  的导数，也就是激活函数在第二层的导数值。在神经网络中，\n",
    "𝑧\n",
    "2\n",
    "z \n",
    "2\n",
    "​\n",
    "  是第二层神经元的加权输入，而 \n",
    "𝑎\n",
    "2\n",
    "=\n",
    "𝜎\n",
    "(\n",
    "𝑧\n",
    "2\n",
    ")\n",
    "a \n",
    "2\n",
    "​\n",
    " =σ(z \n",
    "2\n",
    "​\n",
    " ) 是经过激活函数（如sigmoid函数）后的输出。\n",
    "\n",
    "在使用sigmoid作为激活函数的情况下，其导数形式为 \n",
    "𝜎\n",
    "′\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "=\n",
    "𝜎\n",
    "(\n",
    "𝑧\n",
    ")\n",
    "(\n",
    "1\n",
    "−\n",
    "𝜎\n",
    "(\n",
    "𝑧\n",
    ")\n",
    ")\n",
    "σ \n",
    "′\n",
    " (z)=σ(z)(1−σ(z))。因此，a_2_z_2_grad 实际上是每一项 a_2_i 对应的 z_2_i 的导数，其中 \n",
    "𝑖\n",
    "i 表示第二层神经元的索引。\n",
    "\n",
    "在反向传播算法中，a_2_z_2_grad 的作用是帮助计算损失函数对 \n",
    "𝑊\n",
    "1\n",
    "W \n",
    "1\n",
    "​\n",
    "  的梯度。具体而言，当从输出层往回传播误差信号时，到达第二层时需要知道 \n",
    "𝑧\n",
    "2\n",
    "z \n",
    "2\n",
    "​\n",
    "  的变化如何影响 \n",
    "𝑎\n",
    "2\n",
    "a \n",
    "2\n",
    "​\n",
    " ，进而影响更高层的加权输入和最终的损失函数。因此，a_2_z_2_grad 被用来与从更高层传下来的误差信号（例如 J_a_2_grad）相乘，得到 \n",
    "𝑧\n",
    "2\n",
    "z \n",
    "2\n",
    "​\n",
    "  的梯度 J_z_2_grad，最后通过与输入 \n",
    "𝑋\n",
    "X 的转置相乘得到 \n",
    "𝑊\n",
    "1\n",
    "W \n",
    "1\n",
    "​\n",
    "  的梯度 J_W_1_grad。\n",
    "\n",
    "简而言之，a_2_z_2_grad 在反向传播中起到了连接激活函数导数和损失函数梯度的作用，帮助我们计算出更新网络底层权重所需的梯度信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50b6f3419b5445",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ca8979e98f1957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T04:35:15.141808Z",
     "start_time": "2024-06-16T04:35:15.132043Z"
    }
   },
   "outputs": [],
   "source": [
    "# LeNet5：手写识别网络\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7690915ce813799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        self.convnet = nn.Sequential(OrderedDict([\n",
    "            ('c1', nn.Conv2d(1, 6, kernel_size=(5,5))),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('s2', nn.MaxPool2d(kernel_size=(2,2), stride=2)),\n",
    "            ('c3', nn.Conv2d(6,16,kernel_size=(5,5))),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('s4', nn.MaxPool2d(kernel_size=(2,2), stride=2)),\n",
    "            ('c5', nn.Conv2d(16, 120, kernel_size=(5,5))),\n",
    "            ('relu5', nn.ReLU())\n",
    "        ]))\n",
    "        \n",
    "        self.fc = nn.Sequential(OrderedDict([\n",
    "            ('fc6', nn.Linear(120, 84)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('fc7', nn.Linear(84,10)),\n",
    "            ('logsoftmax', nn.LogSoftmax(dim=-1))\n",
    "        ]))\n",
    "        \n",
    "    def forward(self, img):\n",
    "        output = self.covnet(img)\n",
    "        output = output.view(img.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a49e65f3000e4c",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b99ac3a02661edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-16T05:00:07.746586Z",
     "start_time": "2024-06-16T05:00:07.735959Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN_net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN_net, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        combined = torch.cat((input_, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1c02b1c0e5560f",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16954d8a7aa9e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM_net, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.hidden2tag = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        lstm_out, hidden = self.lstm(input_.view(len(input_), 1, -1), hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(input_), -1))\n",
    "        tag_scores = self.softmax(tag_space)\n",
    "        return tag_scores, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size),\n",
    "                torch.zeros(1, 1, self.hidden_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
