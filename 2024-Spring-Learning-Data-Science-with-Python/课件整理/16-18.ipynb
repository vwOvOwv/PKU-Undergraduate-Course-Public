{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2522ba4e0ec76585",
   "metadata": {},
   "source": [
    "## Review 16-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f435afabb48dcfe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T01:45:39.668335Z",
     "start_time": "2024-06-09T01:45:36.965138Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b629eda5d87a57cc",
   "metadata": {},
   "source": [
    "# Lecture 16: 神经网络基础-Numpy实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38996cc10717abd2",
   "metadata": {},
   "source": [
    "## 1. 人工智能与神经网络的历史\n",
    "- 神经元网络模型的提出\n",
    "- 多层感知机方案能模拟任何逻辑算子\n",
    "- 启动了第一轮人工智能浪潮"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801f12e0627b42d",
   "metadata": {},
   "source": [
    "## 2. 函数回归与梯度下降法\n",
    "- 神经元拟合逻辑计算\n",
    "- 最小二乘拟合与函数回归\n",
    "- 单神经元：Sigmoid函数\n",
    "- 单元神经网络拟合布尔逻辑\n",
    "- 多层网络实现XOR运算\n",
    "- 单神经元网络的局限性：XOR问题\n",
    "- 多元函数梯度下降回归\n",
    "- 多元函数MSE loss\n",
    "- 多元函数偏导与梯度向量\n",
    "- 随机梯度下降法求解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb633aab080ed05",
   "metadata": {},
   "source": [
    "## 3. 单层神经网络回归\n",
    "- 线性函数最小二乘回归\n",
    "- 梯度下降法\n",
    "- MSE loss的偏导与梯度向量（batch size = m）\n",
    "- 梯度下降回归\n",
    "- 生成批量数据\n",
    "- 随机梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd503942a726f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# x为数据，未给出\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X \u001b[38;5;241m=\u001b[39m x[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m      7\u001b[0m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X, X)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# 最小二乘回归\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "print(model)\n",
    "# x为数据，未给出\n",
    "X = x[:, np.newaxis]\n",
    "X.shape\n",
    "\n",
    "model.fit(X, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752e648300be66d",
   "metadata": {},
   "source": [
    "## 4. 多层神经网络建模与反向梯度传播训练\n",
    "- 神经元网络模型回归\n",
    "- 最小化loss，最大似然\n",
    "- 最大后验概率\n",
    "- Logistic Regression\n",
    "- 梯度下降法：计算损失函数的梯度函数\n",
    "- 多分类问题: 矩阵降维 + 激活函数 → MSE\n",
    "- 多分类问题：$X = \\{ x1, x2, x3, x4 \\} → Y = \\{′cat′,′ dog′,′ chicken′\\}$\n",
    "- One-hot encoding：$Y = \\{1,0,0\\}, \\{0,1,0\\}, \\{0,0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b73909c9ae13f",
   "metadata": {},
   "source": [
    "### 网络结构：\n",
    "- $o_1 = x1w_{11} + x2w_{12} + x3w_{13} + x4w_{14} + b1$\n",
    "- $o_2 = x1w_{21} + x2w_{22} + x3w_{23} + x4w_{24} + b2$\n",
    "- $o_3 = x1w_{31} + x2w_{32} + x3w_{33} + x4w_{34} + b3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a121557606407352",
   "metadata": {},
   "source": [
    "### Softmax推断与交叉熵loss\n",
    "- Softmax运算：\n",
    "- Softmax推断：\n",
    "- Softmax与交叉熵损失函数\n",
    "- 熵 (Entropy) 与交叉熵 (Cross-Entropy)\n",
    "- 熵：$H(P) = -\\sum_j P_j \\log P_j$\n",
    "- 交叉熵：$H(P,Q) = -\\sum_j P_j \\log Q_j - Q_j \\log P(j)$\n",
    "- 交叉熵损失函数：\n",
    "- logistic回归对参数求偏导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40dbac855802972",
   "metadata": {},
   "source": [
    "## 5. 用numpy实现一个神经网络\n",
    "- 神经网络的向前传播机制（predict）\n",
    "  - 输入层输入特征\n",
    "  - 按模型权重与偏置量实现向前传播\n",
    "  - Softmax得到结果分布\n",
    "- 单层前馈网络：\n",
    "  - 输入变量: $a= [a1, a2, a3]$\n",
    "  - 权值: $W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} \\\\ w_{21} & w_{22} & w_{23} \\end{bmatrix}$\n",
    "  - 输出: $Z = [z1, z2] = g(W \\cdot a)$\n",
    "  - 权值是通过训练得到的。\n",
    "- 多层神经网络\n",
    "  - $a^{(1)}, a^{(2)}, z$是网络中传输的向量数据\n",
    "  - $b^{(1)}, b^{(2)}, W^{(1)}和W^{(2)}$是网络的参数\n",
    "  - $a^{(2)} = g(W^{(1)} \\cdot a^{(1)} + b^{(1)})$\n",
    "  - $Z = g(W^{(2)} \\cdot a^{(2)} + b^{(2)})$\n",
    "- Sigmoid激活函数\n",
    "\n",
    "### 前馈神经网络（Feedforward Networks）\n",
    "- 向前传播的计算流\n",
    "- 矩阵数据（batch）进行前馈计算\n",
    "\n",
    "### 多层BP神经网络模型学习\n",
    "- 多层多分类网络与激活函数\n",
    "- 反向传播学习\n",
    "\n",
    "### 模型学习流程 - 反向梯度传播\n",
    "- 模型学习流程：\n",
    "  - 向前传播：矩阵计算 + 激活函数 + pooling + norm\n",
    "  - 计算loss\n",
    "  - 反向梯度回传回归模型参数\n",
    "- 链式法则\n",
    "- 反向传播与梯度下降训练法\n",
    "\n",
    "### 假设损失函数为：$$MSE = \\frac{(y - p)^2}{2}; p = g(W \\cdot x + b)$$\n",
    "- 目标：求得一组参数，使得MSE最小。(最优化问题)\n",
    "- W和b更新算法：\n",
    "  - 计算W和b的梯度：$\\Delta W = \\frac{\\partial MSE}{\\partial W}, \\Delta b = \\frac{\\partial MSE}{\\partial b}$\n",
    "  - $W = W - \\lambda \\cdot \\Delta W$\n",
    "  - $b = b - \\lambda \\cdot \\Delta b$($\\lambda$为学习率)\n",
    "- 循环上述过程，直到损失函数足够小。\n",
    "- 批量梯度：每轮权重更新所有样本都参与训练\n",
    "- 随机梯度下降：每轮权重更新只随机选取一个样本参与训练\n",
    "- 梯度下降法与局部最优解\n",
    "- 训练过程：向前传播-计算loss-反向梯度优化\n",
    "\n",
    "### 向前传播（模型计算）\n",
    "- 多层网络计算流程符号体系\n",
    "\n",
    "### 反向梯度传播思想\n",
    "- 从后向前计算\n",
    "- 每层梯度计算都可以看作是一个独立的网络，链式法则\n",
    "- $L-1$层梯度的计算与$L$层的梯度计算有关\n",
    "- 反向传播推导与总结\n",
    "- 反向传播计算流程\n",
    "- 反向传播梯度下降权重修正计算公式\n",
    "\n",
    "## 6. 例子：数据生成与网络模型\n",
    "- 网络模型：2层网络，Sigmoid激活函数\n",
    "- 激活函数求导\n",
    "  - Sigmoid激活函数求导\n",
    "- 定义向前传播及反向计算梯度函数\n",
    "- 模型初始化及训练\n",
    "\n",
    "### 梯度消失与激活函数选择\n",
    "- 模型拟合与冗余\n",
    "- 梯度消失问题\n",
    "- 常见的激活函数\n",
    "  - Sigmoid\n",
    "    - $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "    - $f'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\frac{1}{1 + e^{-x}} - \\frac{1}{(1 + e^{-x})^2}$\n",
    "  - Tanh\n",
    "    - $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "    - $f'(x) = \\frac{(e^x + e^{-x})^2 - (e^x - e^{-x})^2}{(e^x + e^{-x})^2} = 1 - f(x)^2$\n",
    "  - Relu\n",
    "    - $f(x) = \\begin{cases} 0 & x < 0 \\\\ x & x \\geq 0 \\end{cases}$\n",
    "    - $f'(x) = \\begin{cases} 0 & x < 0 \\\\ 1 & x \\geq 0 \\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f188806bfaa60e",
   "metadata": {},
   "source": [
    "# Lecture 18: 神经网络基础-PyTorch的NN架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c39081d39d6c2",
   "metadata": {},
   "source": [
    "# 神经网络基础与PyTorch的nn架构复习笔记\n",
    "\n",
    "## 1. 神经网络模型基础\n",
    "\n",
    "### 1.1 多层神经网络学习\n",
    "- **多层多分类网络与激活函数**\n",
    "- **反向传播学习**\n",
    "\n",
    "### 1.2 模型学习流程 - 反向梯度传播\n",
    "1. **向前传播**：矩阵计算 + 激活函数\n",
    "2. **计算损失函数** (例如：均方误差 MSE)\n",
    "3. **反向梯度回传**：使用链式法则计算梯度，并回归模型参数\n",
    "4. **更新参数**：使用梯度下降法更新权重和偏置\n",
    "\n",
    "### 1.3 反向梯度传播与梯度下降训练法\n",
    "- **损失函数**: $ \\text{MSE} = \\frac{(y - p)^2}{2} $, 其中 $ p = g(W \\cdot x + b) $\n",
    "- **目标**: 最小化 MSE\n",
    "- **参数更新**:\n",
    "  - 计算梯度: $ \\Delta W = \\frac{\\partial \\text{MSE}}{\\partial W} $, $ \\Delta b = \\frac{\\partial \\text{MSE}}{\\partial b} $\n",
    "  - 更新公式: \n",
    "    - $ W = W - \\lambda \\cdot \\Delta W $\n",
    "    - $ b = b - \\lambda \\cdot \\Delta b $ （$ \\lambda $ 为学习率）\n",
    "\n",
    "## 2. 常见激活函数及其导数\n",
    "\n",
    "### 2.1 Sigmoid\n",
    "- **公式**: $ f(x) = \\frac{1}{1 + e^{-x}} $\n",
    "- **导数**: $ f'(x) = f(x) \\cdot (1 - f(x)) $\n",
    "\n",
    "### 2.2 Tanh\n",
    "- **公式**: $ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "- **导数**: $ f'(x) = 1 - f(x)^2 $\n",
    "\n",
    "### 2.3 ReLU\n",
    "- **公式**: $ f(x) = \\max(0, x) $\n",
    "- **导数**: \n",
    "  - $ f'(x) = 0 $ if $ x < 0 $\n",
    "  - $ f'(x) = 1 $ if $ x \\geq 0 $\n",
    "\n",
    "## 3. PyTorch的基本运算\n",
    "\n",
    "### 3.1 Tensor的声明与初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94fa12e5c57c2c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:48:28.124827Z",
     "start_time": "2024-06-09T07:48:26.281352Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 初始化一个tensor\n",
    "x = torch.tensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb594111fda19e7",
   "metadata": {},
   "source": [
    "### 3.2 Tensor的运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da185d322e8476c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:48:44.182923Z",
     "start_time": "2024-06-09T07:48:44.152263Z"
    }
   },
   "outputs": [],
   "source": [
    "# 基本运算示例\n",
    "y = x + 2\n",
    "z = x * 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9c76ca0e8cbd9",
   "metadata": {},
   "source": [
    "### 3.3 使用GPU加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca221bfccd74b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:49:05.288211Z",
     "start_time": "2024-06-09T07:49:05.284215Z"
    }
   },
   "outputs": [],
   "source": [
    "# 将tensor移至GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "x = x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f907fe0cc9ff763",
   "metadata": {},
   "source": [
    "### 4. 神经网络模型设计基本框架\n",
    "#### 4.1 模型层定义\n",
    "+ 线性(全连接)层\n",
    "+ 非线性激活层\n",
    "+ 损失函数设定\n",
    "#### 4.2 优化器\n",
    "+ 学习率与学习策略\n",
    "+ 常用优化器：SGD, Adam\n",
    "#### 4.3 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b211a1eb8b9d37d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-09T07:50:14.444287Z",
     "start_time": "2024-06-09T07:49:55.215514Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 数据变换与增强\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957e4ee0720e4b9",
   "metadata": {},
   "source": [
    "### 5. 卷积神经网络 (CNN)\n",
    "#### 5.1 基本架构\n",
    "+ 卷积层 (nn.Conv2d)\n",
    "+ 激活层 (nn.ReLU)\n",
    "+ 池化层 (nn.MaxPool2d)\n",
    "+ 全连接层 (nn.Linear)\n",
    "+ 输出层 (nn.LogSoftmax 用于多分类)\n",
    "#### 5.2 卷积层定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14585a13a4cf3def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        output = nn.LogSoftmax(dim=1)(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f4e63b8421617",
   "metadata": {},
   "source": [
    "#### 5.3 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff7778028488ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000] Loss: 2.301147\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.412155\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.152496\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.156423\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.091935\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.021849\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.080303\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.023446\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.070930\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.151295\n",
      "Train Epoch: 2 [0/60000] Loss: 0.020539\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.011961\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.019574\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.030182\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.132227\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.004022\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.038181\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.152739\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.068500\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.068467\n",
      "Train Epoch: 3 [0/60000] Loss: 0.020998\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.001969\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.085834\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.061977\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     train(model, device, trainloader, optimizer, epoch)\n",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, trainloader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m---> 14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = SimpleCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, device, trainloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(trainloader.dataset)}] Loss: {loss.item():.6f}')\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, trainloader, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec151ed80ba2d10",
   "metadata": {},
   "source": [
    "### 6. 评价指标\n",
    "+ 准确率 (Accuracy): 预测正确的样本比例\n",
    "+ 精确率 (Precision): 预测为正的样本中正确预测的比例\n",
    "+ 召回率 (Recall): 实际为正的样本中被正确预测的比例\n",
    "+ F1-Measure: 精确率和召回率的调和均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec227f167d748289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate(model, device, testloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    precision = precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = recall_score(all_targets, all_preds, average='macro')\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(testloader.dataset)} ({accuracy * 100:.2f}%)')\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cca91e880e7d7f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Average loss: 0.0000, Accuracy: 9894/10000 (98.94%)\n",
      "Precision: 0.9896, Recall: 0.9892, F1-Score: 0.9893\n"
     ]
    }
   ],
   "source": [
    "# 加载测试数据\n",
    "testset = datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# 评估模型\n",
    "evaluate(model, device, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa88e90a302927b",
   "metadata": {},
   "source": [
    "### 7. 正则化与优化\n",
    "#### 7.1 正则化\n",
    "+ L1 正则化: $ L(w) = \\text{Loss} + \\lambda |w|_1 $\n",
    "+ L2 正则化: $ L(w) = \\text{Loss} + \\lambda |w|_2 $\n",
    "#### 7.2 优化器\n",
    "+ SGD\n",
    "+ AdaGrad\n",
    "+ RMSProp\n",
    "+ Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "184df62a8a59a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Adam优化器并加上L2正则化\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e3309ff6399b3",
   "metadata": {},
   "source": [
    "#### 7.3 数据增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d2961059ce844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
